Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 8
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	train_model
	1

[Mon Apr 22 18:08:55 2024]
rule train_model:
    input: data/drosophila_melanogaster_genes_exons.bed
    output: outputs/245/trained_model.txt, outputs/245/cross_entropy.csv, outputs/245/data_vs_training.svg, outputs/245/metrics.txt
    jobid: 0
    wildcards: id=245

[Mon Apr 22 18:09:20 2024]
Error in rule train_model:
    jobid: 0
    output: outputs/245/trained_model.txt, outputs/245/cross_entropy.csv, outputs/245/data_vs_training.svg, outputs/245/metrics.txt
    shell:
        (/usr/bin/time -f "%e %M" python3 train_model.py data/drosophila_melanogaster_genes_exons.bed         -o outputs/245         -architecture combined         -ns 2          --fit 'i'         --opt_options 1e-06 1e-06 15000 10         --no_mean True         --threads 20          --training_opt 4 2000 95   ) 2>&1 | tail -n1 > outputs/245/metrics.txt
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Removing output files of failed job train_model since they might be corrupted:
outputs/245/metrics.txt
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: /home/hade/Desktop/bc_thesis/.snakemake/log/2024-04-22T180855.063464.snakemake.log
