Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 8
Rules claiming more threads will be scaled down.
Job stats:
job            count
-----------  -------
train_model        1
total              1

Select jobs to execute...
Execute 1 jobs...

[Sat Apr 20 16:37:39 2024]
localrule train_model:
    input: data/bimodal.npy
    output: outputs/67/trained_model.txt, outputs/67/cross_entropy.csv, outputs/67/data_vs_training.svg, outputs/67/metrics.txt
    jobid: 0
    reason: Code has changed since last execution
    wildcards: id=67
    resources: tmpdir=/tmp

[Sat Apr 20 16:37:40 2024]
Error in rule train_model:
    jobid: 0
    input: data/bimodal.npy
    output: outputs/67/trained_model.txt, outputs/67/cross_entropy.csv, outputs/67/data_vs_training.svg, outputs/67/metrics.txt
    shell:
        (/usr/bin/time -f "%e %M" python3 train_model.py data/bimodal.npy         -o outputs/67         -architecture chain         -ns 10          --fit 'd'         --opt_options 1e-06 1e-06 15000 10         --no_mean True         --threads {"ftol": 1e-6,"gtol": 1e-6,"maxiter": 15000,"maxcor": 10}          --training_opt 4 1000 100   ) 2>&1 | tail -n1 > outputs/67/metrics.txt
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Removing output files of failed job train_model since they might be corrupted:
outputs/67/metrics.txt
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: .snakemake/log/2024-04-20T163739.187109.snakemake.log
WorkflowError:
At least one job did not complete successfully.
